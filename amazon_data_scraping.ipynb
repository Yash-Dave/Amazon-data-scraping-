{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "efb66KVa9h9s"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape product listing pages\n",
        "def s_list_p(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "    for product in products:\n",
        "        product_url = product.find('a', {'class': 'a-link-normal s-no-outline'})['href']\n",
        "        product_name = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'}).text.strip()\n",
        "        product_price = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
        "        rating = product.find('span', {'class': 'a-icon-alt'}).text.strip().split()[0]\n",
        "        num_reviews = product.find('span', {'class': 'a-size-base'}).text.strip()\n",
        "\n",
        "        # Write data to CSV file\n",
        "        writer.writerow([product_url, product_name, product_price, rating, num_reviews])\n",
        "\n",
        "# Function to scrape individual product pages\n",
        "def s_p_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    description = soup.find('div', {'id': 'productDescription'}).text.strip()\n",
        "    asin = soup.find('th', text='ASIN').find_next_sibling('td').text.strip()\n",
        "    product_description = soup.find('h1', {'id': 'title'}).text.strip()\n",
        "    manufacturer = soup.find('th', text='Manufacturer').find_next_sibling('td').text.strip()\n",
        "\n",
        "    # Write data to CSV file\n",
        "    writer.writerow([url, description, asin, product_description, manufacturer])\n",
        "\n",
        "# Scrape product listing pages\n",
        "l_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
        "num_pages = 30\n",
        "\n",
        "with open('amazon.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "\n",
        "    for page in range(1, num_pages+1):\n",
        "        s_list_p(l_url + str(page))\n",
        "\n",
        "\n",
        "p_urls = []\n",
        "\n",
        "with open('amazon.csv', 'a', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    for url in product_urls:\n",
        "        s_p_page(url)\n"
      ]
    }
  ]
}